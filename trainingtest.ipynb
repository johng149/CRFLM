{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import CustomModelNoDownsize as CustomModel\n",
    "from models.model import CRFModelV2 as CRFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenmonster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = \"english-8000-balanced-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenmonster.load_multiprocess_safe(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_vocab_size is 8000, so largest valid index is 7999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = initial_vocab_size # max valid index is now 8000\n",
    "eos_idx = initial_vocab_size + 1 # max valid index is now 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = initial_vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len = 256\n",
    "context_len = 512\n",
    "answer_len = 100\n",
    "assert context_len >= answer_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "num_helix_layers = 1\n",
    "num_single_strand_layers = 1\n",
    "phm_factor = 4\n",
    "lm_head_phm_factor = 2\n",
    "beam = 32\n",
    "low_rank = 16\n",
    "batch_size = 52\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{checkpoint_dir}/crf_model_test.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_file = \"data/squad_train\"\n",
    "dolly_test_file = \"data/closed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_from_disk(squad_train_file)\n",
    "dolly = load_from_disk(dolly_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, question_key, context_key, answer_key):\n",
    "    \"\"\"\n",
    "    Each batch has 3 elements: instruction, context, response\n",
    "    However since their lengths may vary, we need to pad them.\n",
    "\n",
    "    For response, must ensure that we add eos token and then begin padding.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    answers = []\n",
    "\n",
    "    for elem in batch:\n",
    "        i = elem[question_key]\n",
    "        c = elem[context_key]\n",
    "        r = elem[answer_key]\n",
    "\n",
    "        c = torch.cat((i, c))\n",
    "        contexts.append(c)\n",
    "        answers.append(r)\n",
    "\n",
    "\n",
    "    # Pad contexts to context_len\n",
    "    contexts = [pad(c, (0, (context_len+question_len) - len(c)), value=pad_idx) for c in contexts]\n",
    "\n",
    "    # Pad responses to answer_len\n",
    "    # though first making sure that eos token is added to end of each response\n",
    "    # eos_append = torch.tensor([eos_idx])\n",
    "    # answers = [torch.cat((r[:answer_len-1], eos_append)) for r in answers]\n",
    "    eos_append = torch.tensor([eos_idx, eos_idx])\n",
    "    answers = [torch.cat((r[:answer_len-2], eos_append)) for r in answers]\n",
    "    answers = [pad(r, (0, answer_len - len(r)), value=pad_idx) for r in answers]\n",
    "\n",
    "    return torch.stack(contexts), torch.stack(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dolly_collate_fn(batch):\n",
    "    return collate_fn(batch, \"instruction\", \"context\", \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_collate_fn(batch):\n",
    "    return collate_fn(batch, \"question\", \"context\", \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollyDataloader = DataLoader(dolly, batch_size=batch_size, shuffle=True, collate_fn=dolly_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadDataloader = DataLoader(squad, batch_size=batch_size, shuffle=True, collate_fn=squad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in squadDataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(losses, model, crf, optm, tensorboard_log_dir):\n",
    "    # first check to see if checkpoint dir exists, if not, create it\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "    'epoch': len(losses),\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optm.state_dict(),\n",
    "    'loss': losses[-1],\n",
    "    'losses': losses,\n",
    "    'crf_state_dict': crf.state_dict(),\n",
    "    'model_kwargs': model.kwargs,\n",
    "    'crf_kwargs': crf.kwargs,\n",
    "    'tensorboard_log_dir': tensorboard_log_dir\n",
    "    }, path)\n",
    "def load_checkpoint(map_location=None):\n",
    "    checkpoint = torch.load(path, map_location=map_location)\n",
    "    model = CustomModel(**checkpoint['model_kwargs'])\n",
    "    crf = CRFModel(model=model, **checkpoint['crf_kwargs'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    crf.load_state_dict(checkpoint['crf_state_dict'])\n",
    "    model = model.to(device)\n",
    "    crf = crf.to(device)\n",
    "    optm = torch.optim.Adam(crf.parameters())\n",
    "    optm.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    losses = checkpoint['losses']\n",
    "    log_dir = checkpoint['tensorboard_log_dir']\n",
    "    return losses, model, crf, optm, log_dir\n",
    "def try_loading():\n",
    "    \"\"\"\n",
    "    First try to load the model, if it doesn't exist, create one\n",
    "    based on the parameters specified above\n",
    "    \"\"\"\n",
    "    try:\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint()\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except FileNotFoundError:\n",
    "        # couldn't find model, probably because it doesn't exist\n",
    "        print(\"Couldn't find model, creating new one\")\n",
    "        model = CustomModel(embedding_dim, num_heads, vocab_size, num_helix_layers=num_helix_layers, num_single_strand_layers=num_single_strand_layers, phm_factor=phm_factor, lm_head_phm_factor=lm_head_phm_factor)\n",
    "        model = model.to(device)\n",
    "        crf = CRFModel(model, vocab_size, beam, low_rank, pad_idx)\n",
    "        crf = crf.to(device)\n",
    "        optm = Adam(crf.parameters(), lr=0.001)\n",
    "        losses = []\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # create a string to identify this model for tensorboard logging\n",
    "        now = datetime.datetime.now()\n",
    "        log_dir = f\"runs/run_at_{now.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except RuntimeError:\n",
    "        # probably because model was saved on gpu and now we're using cpu\n",
    "        # so can still load it, but need to specify map_location\n",
    "        print(\"Model found but was saved on gpu, attempting to load on cpu\")\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint(map_location='cpu')\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_functions.unlikelihood_loss import unlikelihood_loss\n",
    "from loss_functions.nag_bert_loss import nag_bert_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nll_loss_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming, have seen 15000 epochs\n",
      "Have 10783810 trainable parameters\n",
      "Logging to runs/run_at_2023-11-25_15-44-28\n"
     ]
    }
   ],
   "source": [
    "losses, model, crf, optm, log_dir = try_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, targets, crf_losses):\n",
    "    bt_size, _ = targets.shape\n",
    "    cross_entropy_loss = torch.nn.functional.cross_entropy(logits.reshape(-1, vocab_size), targets.view(-1), ignore_index=pad_idx, reduction='none').view(bt_size, answer_len)\n",
    "    target_padding_matrix = ~targets.eq(pad_idx)\n",
    "    target_padding_matrix = target_padding_matrix.type(cross_entropy_loss.type())\n",
    "    cross_entropy_loss = cross_entropy_loss * target_padding_matrix\n",
    "    summed_loss = cross_entropy_loss.sum(dim = -1)\n",
    "    one_step_train_loss = crf_losses + nll_loss_weight * summed_loss\n",
    "    scaled_train_loss = torch.sum(one_step_train_loss) / torch.sum(target_padding_matrix)\n",
    "    return scaled_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(c_sample):\n",
    "    with torch.no_grad():\n",
    "        model_input_sample = torch.full_like(r_sample, pad_idx)\n",
    "        scores, tokens = crf.inference(c_sample, model_input_sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_decode(tokens):\n",
    "    # differs from vocab.decode in that it stops decoding when it sees eos_idx or pad_idx\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if t == eos_idx or t == pad_idx:\n",
    "            break\n",
    "        else:\n",
    "            filtered_tokens.append(t)\n",
    "    return vocab.decode(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import adam optimizer\n",
    "from torch.optim import Adam\n",
    "optm = Adam(crf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadIter = iter(squadDataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have seen 15000 epochs, training for 2000 more for a total of 17000\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 20000\n",
    "seen = len(losses)\n",
    "epochs = total_epochs - seen\n",
    "print(f\"Have seen {seen} epochs, training for {epochs} more for a total of {total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cbd88181a34745b2520dbe2d6c062a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    try:\n",
    "        batch = next(squadIter)\n",
    "    except StopIteration:\n",
    "        squadIter = iter(squadDataloader)\n",
    "        batch = next(squadIter)\n",
    "    c, r = batch\n",
    "    c = c.to(device)\n",
    "    r = r.to(device)\n",
    "    model_input = torch.full_like(r, pad_idx)\n",
    "    logits, crf_losses = crf(c, model_input, r)\n",
    "    loss = custom_loss(logits, r, crf_losses)\n",
    "    optm.zero_grad()\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    losses.append(loss.item())\n",
    "    writer.add_scalar(\"Loss/train\", losses[-1], epoch + seen)\n",
    "    pbar.set_description(f\"Epoch {epoch + seen} of {total_epochs}, loss: {loss.item()}\")\n",
    "    if epoch % 20 == 0:\n",
    "        save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample of c and r to test model generation\n",
    "sample_i = 23\n",
    "assert sample_i < batch_size\n",
    "c_sample = c[sample_i:sample_i+1]\n",
    "r_sample = r[sample_i:sample_i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = inference(c_sample).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[332,\n",
       " 2259,\n",
       " 45,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 1263,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 461,\n",
       " 37,\n",
       " 719,\n",
       " 37,\n",
       " 719,\n",
       " 37,\n",
       " 719,\n",
       " 37,\n",
       " 719,\n",
       " 37,\n",
       " 719,\n",
       " 37,\n",
       " 648,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 6252,\n",
       " 1055,\n",
       " 394,\n",
       " 37,\n",
       " 1765,\n",
       " 454,\n",
       " 394,\n",
       " 6252,\n",
       " 253,\n",
       " 514,\n",
       " 6252,\n",
       " 253,\n",
       " 514,\n",
       " 6252,\n",
       " 253,\n",
       " 454,\n",
       " 110,\n",
       " 505,\n",
       " 1985,\n",
       " 514,\n",
       " 3778,\n",
       " 8001,\n",
       " 1996,\n",
       " 4653,\n",
       " 1890,\n",
       " 2004,\n",
       " 5088,\n",
       " 2189,\n",
       " 6164,\n",
       " 1922,\n",
       " 71,\n",
       " 7620]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What country did kids play football in public schools in the 19th century?Association football in itself does not have a classical history. Notwithstanding any similarities to other ball games played around the world FIFA have recognised that no historical connection exists with any game played in antiquity outside Europe. The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England. The history of football in England dates back to at least the eighth century AD.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(c_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'China'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'England'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(r_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input = 'On what date is the Feat of Transfiguration celebrated?Ecclesiam suam was given at St. Peter\\'s, Rome, on the Feast of the Transfiguration, 6 August 1964, the second year of his Pontificate. It is considered an important document, identifying the Catholic Church with the Body of Christ. A later Council document Lumen Gentium stated that the Church subsists in the Body of Christ, raising questions as to the difference between \"is\" and \"subsists in\". Paul VI appealed to \"all people of good will\" and discussed necessary dialogues within the Church and between the Churches and with atheism.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = vocab.tokenize(custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 332,  772, 2777, 2291, 4165,   36,  233,   37, 1413,  769,   36,\n",
       "       3604,   37,  705,   37,  716, 3804, 6446,   48,   34,  332,  688,\n",
       "         37, 1351,  500,  348,  806,  348, 1835, 3242,  655, 3761, 1653,\n",
       "        387,  275,  290,  791,  445,   15, 4287,   36,  704,   37, 1278,\n",
       "       4280,   36, 3604,   37,  705,   37,  716, 3804,   15,  216, 5497,\n",
       "       1234,   23,   15, 7222, 2796, 4277,   36, 1665,  509,  392,  992,\n",
       "       1003, 3312, 7015, 7582, 5762,   15, 5870, 4694, 6871, 5502, 6232,\n",
       "         36, 2232,  769, 5501, 1914, 3325, 6294, 5762, 1950, 1086,   36,\n",
       "       1473,  509,  517, 4441, 6133, 5502, 2705, 2072, 4153,   36, 2232,\n",
       "        769, 5501,   15, 1683, 2110, 6683, 3073, 1788, 7950,  580,  729,\n",
       "       2806,  580, 2705, 2072,  727,  850,  777,  516,   38,  828, 3881,\n",
       "       2933,  580, 1256, 4308,  769, 2391, 2783, 2806, 6497, 6631, 1388,\n",
       "         37, 1569,  515,   63, 7258, 5502, 1263, 7345, 5502,  388, 5654,\n",
       "        655,   37, 1498,   37,  800,   17], dtype=uint16)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded from uint16 to int32\n",
    "encoded = encoded.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = torch.tensor(encoded).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_output = inference(encoded.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 332, 1412,  443,  929, 1854,  929,  362, 4310,  929,  362,  929,  362,\n",
       "          929,  362,  929,  362,  929,  362,  929,  362,  929,  362,  929,  362,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "          727,  485,  485,   70,   37,  773,   37,  773,   37,  773,   37, 1423,\n",
       "          485, 1423,  485,  773,  672, 3788, 1472, 3788,  749, 3788, 1472, 3788,\n",
       "         1472, 3788, 3183, 3788, 3183, 7190, 7651,  846, 5711, 6216, 5711, 6216,\n",
       "         5711, 2141, 3030, 7569]], device='cuda:0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Early B – BCE period BCE BCE BCE BCE BCE BCE BCE BCE'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(custom_output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

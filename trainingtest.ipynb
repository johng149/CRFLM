{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import CustomModelNoDownsize as CustomModel\n",
    "from models.model import CRFModelV2 as CRFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenmonster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = \"english-8000-balanced-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenmonster.load_multiprocess_safe(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_vocab_size is 8000, so largest valid index is 7999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = initial_vocab_size # max valid index is now 8000\n",
    "eos_idx = initial_vocab_size + 1 # max valid index is now 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = initial_vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len = 256\n",
    "context_len = 512\n",
    "answer_len = 100\n",
    "assert context_len >= answer_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "num_helix_layers = 1\n",
    "num_single_strand_layers = 1\n",
    "phm_factor = 4\n",
    "lm_head_phm_factor = 2\n",
    "beam = 32\n",
    "low_rank = 16\n",
    "batch_size = 52\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{checkpoint_dir}/crf_model_test.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_file = \"data/squad_train\"\n",
    "dolly_test_file = \"data/closed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_from_disk(squad_train_file)\n",
    "dolly = load_from_disk(dolly_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, question_key, context_key, answer_key):\n",
    "    \"\"\"\n",
    "    Each batch has 3 elements: instruction, context, response\n",
    "    However since their lengths may vary, we need to pad them.\n",
    "\n",
    "    For response, must ensure that we add eos token and then begin padding.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    answers = []\n",
    "\n",
    "    for elem in batch:\n",
    "        i = elem[question_key]\n",
    "        c = elem[context_key]\n",
    "        r = elem[answer_key]\n",
    "\n",
    "        c = torch.cat((i, c))\n",
    "        contexts.append(c)\n",
    "        answers.append(r)\n",
    "\n",
    "\n",
    "    # Pad contexts to context_len\n",
    "    contexts = [pad(c, (0, (context_len+question_len) - len(c)), value=pad_idx) for c in contexts]\n",
    "\n",
    "    # Pad responses to answer_len\n",
    "    # though first making sure that eos token is added to end of each response\n",
    "    # eos_append = torch.tensor([eos_idx])\n",
    "    # answers = [torch.cat((r[:answer_len-1], eos_append)) for r in answers]\n",
    "    eos_append = torch.tensor([eos_idx, eos_idx])\n",
    "    answers = [torch.cat((r[:answer_len-2], eos_append)) for r in answers]\n",
    "    answers = [pad(r, (0, answer_len - len(r)), value=pad_idx) for r in answers]\n",
    "\n",
    "    return torch.stack(contexts), torch.stack(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dolly_collate_fn(batch):\n",
    "    return collate_fn(batch, \"instruction\", \"context\", \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_collate_fn(batch):\n",
    "    return collate_fn(batch, \"question\", \"context\", \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollyDataloader = DataLoader(dolly, batch_size=batch_size, shuffle=True, collate_fn=dolly_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadDataloader = DataLoader(squad, batch_size=batch_size, shuffle=True, collate_fn=squad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in squadDataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(losses, model, crf, optm, tensorboard_log_dir):\n",
    "    # first check to see if checkpoint dir exists, if not, create it\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "    'epoch': len(losses),\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optm.state_dict(),\n",
    "    'loss': losses[-1],\n",
    "    'losses': losses,\n",
    "    'crf_state_dict': crf.state_dict(),\n",
    "    'model_kwargs': model.kwargs,\n",
    "    'crf_kwargs': crf.kwargs,\n",
    "    'tensorboard_log_dir': tensorboard_log_dir\n",
    "    }, path)\n",
    "def load_checkpoint(map_location=None):\n",
    "    checkpoint = torch.load(path, map_location=map_location)\n",
    "    model = CustomModel(**checkpoint['model_kwargs'])\n",
    "    crf = CRFModel(model=model, **checkpoint['crf_kwargs'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    crf.load_state_dict(checkpoint['crf_state_dict'])\n",
    "    model = model.to(device)\n",
    "    crf = crf.to(device)\n",
    "    optm = torch.optim.Adam(crf.parameters())\n",
    "    optm.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    losses = checkpoint['losses']\n",
    "    log_dir = checkpoint['tensorboard_log_dir']\n",
    "    return losses, model, crf, optm, log_dir\n",
    "def try_loading():\n",
    "    \"\"\"\n",
    "    First try to load the model, if it doesn't exist, create one\n",
    "    based on the parameters specified above\n",
    "    \"\"\"\n",
    "    try:\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint()\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except FileNotFoundError:\n",
    "        # couldn't find model, probably because it doesn't exist\n",
    "        print(\"Couldn't find model, creating new one\")\n",
    "        model = CustomModel(embedding_dim, num_heads, vocab_size, num_helix_layers=num_helix_layers, num_single_strand_layers=num_single_strand_layers, phm_factor=phm_factor, lm_head_phm_factor=lm_head_phm_factor)\n",
    "        model = model.to(device)\n",
    "        crf = CRFModel(model, vocab_size, beam, low_rank, pad_idx)\n",
    "        crf = crf.to(device)\n",
    "        optm = Adam(crf.parameters(), lr=0.001)\n",
    "        losses = []\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # create a string to identify this model for tensorboard logging\n",
    "        now = datetime.datetime.now()\n",
    "        log_dir = f\"runs/run_at_{now.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except RuntimeError:\n",
    "        # probably because model was saved on gpu and now we're using cpu\n",
    "        # so can still load it, but need to specify map_location\n",
    "        print(\"Model found but was saved on gpu, attempting to load on cpu\")\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint(map_location='cpu')\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_functions.unlikelihood_loss import unlikelihood_loss\n",
    "from loss_functions.nag_bert_loss import nag_bert_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nll_loss_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming, have seen 22000 epochs\n",
      "Have 10783810 trainable parameters\n",
      "Logging to runs/run_at_2023-11-25_15-44-28\n"
     ]
    }
   ],
   "source": [
    "losses, model, crf, optm, log_dir = try_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, targets, crf_losses):\n",
    "    bt_size, _ = targets.shape\n",
    "    cross_entropy_loss = torch.nn.functional.cross_entropy(logits.reshape(-1, vocab_size), targets.view(-1), ignore_index=pad_idx, reduction='none').view(bt_size, answer_len)\n",
    "    target_padding_matrix = ~targets.eq(pad_idx)\n",
    "    target_padding_matrix = target_padding_matrix.type(cross_entropy_loss.type())\n",
    "    cross_entropy_loss = cross_entropy_loss * target_padding_matrix\n",
    "    summed_loss = cross_entropy_loss.sum(dim = -1)\n",
    "    one_step_train_loss = crf_losses + nll_loss_weight * summed_loss\n",
    "    scaled_train_loss = torch.sum(one_step_train_loss) / torch.sum(target_padding_matrix)\n",
    "    return scaled_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(c_sample):\n",
    "    with torch.no_grad():\n",
    "        model_input_sample = torch.full_like(r_sample, pad_idx)\n",
    "        scores, tokens = crf.inference(c_sample, model_input_sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_decode(tokens):\n",
    "    # differs from vocab.decode in that it stops decoding when it sees eos_idx or pad_idx\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if t == eos_idx or t == pad_idx:\n",
    "            break\n",
    "        else:\n",
    "            filtered_tokens.append(t)\n",
    "    return vocab.decode(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import adam optimizer\n",
    "from torch.optim import Adam\n",
    "optm = Adam(crf.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadIter = iter(squadDataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have seen 22000 epochs, training for 2000 more for a total of 24000\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 24000\n",
    "seen = len(losses)\n",
    "epochs = total_epochs - seen\n",
    "print(f\"Have seen {seen} epochs, training for {epochs} more for a total of {total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ce8d1ccebd45408a9fea9c332e11bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    try:\n",
    "        batch = next(squadIter)\n",
    "    except StopIteration:\n",
    "        squadIter = iter(squadDataloader)\n",
    "        batch = next(squadIter)\n",
    "    c, r = batch\n",
    "    c = c.to(device)\n",
    "    r = r.to(device)\n",
    "    model_input = torch.full_like(r, pad_idx)\n",
    "    logits, crf_losses = crf(c, model_input, r)\n",
    "    loss = custom_loss(logits, r, crf_losses)\n",
    "    optm.zero_grad()\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    losses.append(loss.item())\n",
    "    writer.add_scalar(\"Loss/train\", losses[-1], epoch + seen)\n",
    "    pbar.set_description(f\"Epoch {epoch + seen} of {total_epochs}, loss: {loss.item()}\")\n",
    "    if epoch % 20 == 0:\n",
    "        save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample of c and r to test model generation\n",
    "sample_i = 38\n",
    "assert sample_i < batch_size\n",
    "c_sample = c[sample_i:sample_i+1]\n",
    "r_sample = r[sample_i:sample_i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = inference(c_sample).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37,\n",
       " 4416,\n",
       " 1555,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 784,\n",
       " 37,\n",
       " 784,\n",
       " 784,\n",
       " 37,\n",
       " 784,\n",
       " 68,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 8001,\n",
       " 68,\n",
       " 65,\n",
       " 37,\n",
       " 242,\n",
       " 37,\n",
       " 242,\n",
       " 37,\n",
       " 242,\n",
       " 37,\n",
       " 242,\n",
       " 65,\n",
       " 333,\n",
       " 242,\n",
       " 333,\n",
       " 242,\n",
       " 523,\n",
       " 36,\n",
       " 797,\n",
       " 333,\n",
       " 242,\n",
       " 523,\n",
       " 36,\n",
       " 797,\n",
       " 110,\n",
       " 333,\n",
       " 242,\n",
       " 523,\n",
       " 3740,\n",
       " 3753,\n",
       " 110,\n",
       " 523,\n",
       " 3740,\n",
       " 128,\n",
       " 333,\n",
       " 1839,\n",
       " 523,\n",
       " 3740,\n",
       " 3753,\n",
       " 110,\n",
       " 333,\n",
       " 1839,\n",
       " 3753,\n",
       " 110,\n",
       " 308,\n",
       " 8001,\n",
       " 3916,\n",
       " 5466,\n",
       " 919,\n",
       " 5466,\n",
       " 3505,\n",
       " 210]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What does Darwin use to illustrate the effects of artificial selection?Chapter I covers animal husbandry and plant breeding, going back to ancient Egypt. Darwin discusses contemporary opinions on the origins of different breeds under cultivation to argue that many have been produced from common ancestors by selective breeding. As an illustration of artificial selection, he describes fancy pigeon breeding, noting that \"[t]he diversity of the breeds is something astonishing\", yet all were descended from one species of rock pigeon. Darwin saw two distinct kinds of variation: (1) rare abrupt changes he called \"sports\" or \"monstrosities\" (example: ancon sheep with short legs), and (2) ubiquitous small differences (example: slightly shorter or longer bill of pigeons). Both types of hereditary changes can be used by breeders. However, for Darwin the small changes were most important in evolution.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(c_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple law'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fancy pigeon breeding'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(r_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input = 'On what date is the Feat of Transfiguration celebrated?Ecclesiam suam was given at St. Peter\\'s, Rome, on the Feast of the Transfiguration, 6 August 1964, the second year of his Pontificate. It is considered an important document, identifying the Catholic Church with the Body of Christ. A later Council document Lumen Gentium stated that the Church subsists in the Body of Christ, raising questions as to the difference between \"is\" and \"subsists in\". Paul VI appealed to \"all people of good will\" and discussed necessary dialogues within the Church and between the Churches and with atheism.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = vocab.tokenize(custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 332,  772, 2777, 2291, 4165,   36,  233,   37, 1413,  769,   36,\n",
       "       3604,   37,  705,   37,  716, 3804, 6446,   48,   34,  332,  688,\n",
       "         37, 1351,  500,  348,  806,  348, 1835, 3242,  655, 3761, 1653,\n",
       "        387,  275,  290,  791,  445,   15, 4287,   36,  704,   37, 1278,\n",
       "       4280,   36, 3604,   37,  705,   37,  716, 3804,   15,  216, 5497,\n",
       "       1234,   23,   15, 7222, 2796, 4277,   36, 1665,  509,  392,  992,\n",
       "       1003, 3312, 7015, 7582, 5762,   15, 5870, 4694, 6871, 5502, 6232,\n",
       "         36, 2232,  769, 5501, 1914, 3325, 6294, 5762, 1950, 1086,   36,\n",
       "       1473,  509,  517, 4441, 6133, 5502, 2705, 2072, 4153,   36, 2232,\n",
       "        769, 5501,   15, 1683, 2110, 6683, 3073, 1788, 7950,  580,  729,\n",
       "       2806,  580, 2705, 2072,  727,  850,  777,  516,   38,  828, 3881,\n",
       "       2933,  580, 1256, 4308,  769, 2391, 2783, 2806, 6497, 6631, 1388,\n",
       "         37, 1569,  515,   63, 7258, 5502, 1263, 7345, 5502,  388, 5654,\n",
       "        655,   37, 1498,   37,  800,   17], dtype=uint16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded from uint16 to int32\n",
    "encoded = encoded.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = torch.tensor(encoded).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_output = inference(encoded.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 332, 1585, 3886,  629,  929,   47, 2527,  457,  517,  929,  362,    3,\n",
       "          311, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "         8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
       "          290, 1424,   70,   36, 1424,  333,   36,  660,  797,   37,  797,  463,\n",
       "          486,  463,  463,  463,  463,  463,  463,  463,  463,  463,  463, 2080,\n",
       "         6102,  463,  463,  463,  463,  463, 2127, 1135,  977, 7014, 3946, 5577,\n",
       "         6017, 6017, 6017, 1677]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'May around 80 BC millnium BCE 15'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(custom_output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

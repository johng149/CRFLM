{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.model import CustomModelNoDownsize as CustomModel\n",
    "# from models.model import CRFModelV2 as CRFModel\n",
    "from models.model import CustomModel\n",
    "from models.model import CRFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenmonster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = \"english-8000-balanced-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenmonster.load_multiprocess_safe(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_vocab_size is 8000, so largest valid index is 7999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = initial_vocab_size # max valid index is now 8000\n",
    "eos_idx = initial_vocab_size + 1 # max valid index is now 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = initial_vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_len = 256\n",
    "context_len = 512\n",
    "answer_len = 100\n",
    "assert context_len >= answer_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "num_helix_layers = 1\n",
    "num_single_strand_layers = 1\n",
    "phm_factor = 4\n",
    "lm_head_phm_factor = 2\n",
    "beam = 32\n",
    "low_rank = 16\n",
    "batch_size = 25\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{checkpoint_dir}/crf_model_reflex.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_file = \"data/squad_train\"\n",
    "dolly_test_file = \"data/closed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_from_disk(squad_train_file)\n",
    "dolly = load_from_disk(dolly_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, question_key, context_key, answer_key):\n",
    "    \"\"\"\n",
    "    Each batch has 3 elements: instruction, context, response\n",
    "    However since their lengths may vary, we need to pad them.\n",
    "\n",
    "    For response, must ensure that we add eos token and then begin padding.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    answers = []\n",
    "\n",
    "    for elem in batch:\n",
    "        i = elem[question_key]\n",
    "        c = elem[context_key]\n",
    "        r = elem[answer_key]\n",
    "\n",
    "        c = torch.cat((i, c))\n",
    "        contexts.append(c)\n",
    "        answers.append(r)\n",
    "\n",
    "\n",
    "    # Pad contexts to context_len\n",
    "    contexts = [pad(c, (0, (context_len+question_len) - len(c)), value=pad_idx) for c in contexts]\n",
    "\n",
    "    # Pad responses to answer_len\n",
    "    # though first making sure that eos token is added to end of each response\n",
    "    # eos_append = torch.tensor([eos_idx])\n",
    "    # answers = [torch.cat((r[:answer_len-1], eos_append)) for r in answers]\n",
    "    eos_append = torch.tensor([eos_idx, eos_idx])\n",
    "    answers = [torch.cat((r[:answer_len-2], eos_append)) for r in answers]\n",
    "    answers = [pad(r, (0, answer_len - len(r)), value=pad_idx) for r in answers]\n",
    "\n",
    "    return torch.stack(contexts), torch.stack(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dolly_collate_fn(batch):\n",
    "    return collate_fn(batch, \"instruction\", \"context\", \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_collate_fn(batch):\n",
    "    return collate_fn(batch, \"question\", \"context\", \"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollyDataloader = DataLoader(dolly, batch_size=batch_size, shuffle=True, collate_fn=dolly_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadDataloader = DataLoader(squad, batch_size=batch_size, shuffle=True, collate_fn=squad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(losses, model, crf, optm, tensorboard_log_dir):\n",
    "    # first check to see if checkpoint dir exists, if not, create it\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "    'epoch': len(losses),\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optm.state_dict(),\n",
    "    'loss': losses[-1],\n",
    "    'losses': losses,\n",
    "    'crf_state_dict': crf.state_dict(),\n",
    "    'model_kwargs': model.kwargs,\n",
    "    'crf_kwargs': crf.kwargs,\n",
    "    'tensorboard_log_dir': tensorboard_log_dir\n",
    "    }, path)\n",
    "def load_checkpoint(map_location=None):\n",
    "    checkpoint = torch.load(path, map_location=map_location)\n",
    "    model = CustomModel(**checkpoint['model_kwargs'])\n",
    "    crf = CRFModel(model=model, **checkpoint['crf_kwargs'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    crf.load_state_dict(checkpoint['crf_state_dict'])\n",
    "    model = model.to(device)\n",
    "    crf = crf.to(device)\n",
    "    optm = torch.optim.Adam(crf.parameters())\n",
    "    optm.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    losses = checkpoint['losses']\n",
    "    log_dir = checkpoint['tensorboard_log_dir']\n",
    "    return losses, model, crf, optm, log_dir\n",
    "def try_loading():\n",
    "    \"\"\"\n",
    "    First try to load the model, if it doesn't exist, create one\n",
    "    based on the parameters specified above\n",
    "    \"\"\"\n",
    "    try:\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint()\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except FileNotFoundError:\n",
    "        # couldn't find model, probably because it doesn't exist\n",
    "        print(\"Couldn't find model, creating new one\")\n",
    "        model = CustomModel(embedding_dim, num_heads, vocab_size, num_helix_layers=num_helix_layers, num_single_strand_layers=num_single_strand_layers, phm_factor=phm_factor, lm_head_phm_factor=lm_head_phm_factor)\n",
    "        model = model.to(device)\n",
    "        crf = CRFModel(model, vocab_size, beam, low_rank, pad_idx)\n",
    "        crf = crf.to(device)\n",
    "        optm = Adam(crf.parameters(), lr=0.001)\n",
    "        losses = []\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        # create a string to identify this model for tensorboard logging\n",
    "        now = datetime.datetime.now()\n",
    "        log_dir = f\"runs/run_at_{now.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir\n",
    "    except RuntimeError:\n",
    "        # probably because model was saved on gpu and now we're using cpu\n",
    "        # so can still load it, but need to specify map_location\n",
    "        print(\"Model found but was saved on gpu, attempting to load on cpu\")\n",
    "        losses, model, crf, optm, log_dir = load_checkpoint(map_location='cpu')\n",
    "        print(f\"Resuming, have seen {len(losses)} epochs\")\n",
    "        print(f\"Have {sum(p.numel() for p in crf.parameters() if p.requires_grad)} trainable parameters\")\n",
    "        print(f\"Logging to {log_dir}\")\n",
    "        return losses, model, crf, optm, log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_functions.unlikelihood_loss import unlikelihood_loss\n",
    "from loss_functions.nag_bert_loss import custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "nll_loss_weight = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming, have seen 12484 epochs\n",
      "Have 10783810 trainable parameters\n",
      "Logging to runs/run_at_2023-11-26_08-06-41\n"
     ]
    }
   ],
   "source": [
    "losses, model, crf, optm, log_dir = try_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(c_sample, model_input_sample=None):\n",
    "    with torch.no_grad():\n",
    "        bs, _ = c_sample.shape\n",
    "        if model_input_sample is None:\n",
    "            model_input_sample = torch.ones(bs, answer_len) * pad_idx\n",
    "            model_input_sample = model_input_sample.long().to(device)\n",
    "        scores, tokens = crf.inference(c_sample, model_input_sample)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_decode(tokens):\n",
    "    # differs from vocab.decode in that it stops decoding when it sees eos_idx or pad_idx\n",
    "    filtered_tokens = []\n",
    "    for t in tokens:\n",
    "        if t == eos_idx or t == pad_idx:\n",
    "            break\n",
    "        else:\n",
    "            filtered_tokens.append(t)\n",
    "    return vocab.decode(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "squadIter = iter(squadDataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have seen 12484 epochs, training for 9516 more for a total of 22000\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 22000\n",
    "seen = len(losses)\n",
    "epochs = total_epochs - seen\n",
    "print(f\"Have seen {seen} epochs, training for {epochs} more for a total of {total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e517a10dac642358d74175037dda495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    try:\n",
    "        batch = next(squadIter)\n",
    "    except StopIteration:\n",
    "        squadIter = iter(squadDataloader)\n",
    "        batch = next(squadIter)\n",
    "    c, r = batch\n",
    "    c = c.to(device)\n",
    "    r = r.to(device)\n",
    "    model_input = torch.full_like(r, pad_idx)\n",
    "    logits, crf_losses = crf(c, model_input, r)\n",
    "    loss = custom_loss(logits, r, crf_losses, vocab_size, pad_idx, nll_loss_weight, answer_len)\n",
    "    # model_input is full of pad_idx, but we also want to train the model\n",
    "    # to be able to correct itself (reflection)\n",
    "    with torch.no_grad():\n",
    "        _, model_prediction = crf.inference(c, model_input)\n",
    "    logits_reflection, crf_losses_reflection = crf(c, model_prediction, r)\n",
    "    loss_reflection = custom_loss(logits_reflection, r, crf_losses_reflection, vocab_size, pad_idx, nll_loss_weight, answer_len)\n",
    "    loss = loss + loss_reflection\n",
    "    optm.zero_grad()\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    losses.append(loss.item())\n",
    "    writer.add_scalar(\"Loss/train\", losses[-1], epoch + seen)\n",
    "    pbar.set_description(f\"Epoch {epoch + seen} of {total_epochs}, loss: {loss.item()}\")\n",
    "    if epoch % 20 == 0:\n",
    "        save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(losses, model, crf, optm, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sample of c and r to test model generation\n",
    "sample_i = 9\n",
    "assert sample_i < batch_size\n",
    "c_sample = c[sample_i:sample_i+1]\n",
    "r_sample = r[sample_i:sample_i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = inference(c_sample)\n",
    "tokensl = tokens.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Were peopel in pre-industrial societies considered to have long or short lifespans?The work of children was important in pre-industrial societies, as children needed to provide their labour for their survival and that of their group. Pre-industrial societies were characterised by low productivity and short life expectancy, preventing children from participating in productive work would be more harmful to their welfare and that of their group in the long run. In pre-industrial societies, there was little need for children to attend school. This is especially the case in non literate societies. Most pre-industrial skill and knowledge were amenable to being passed down through direct mentoring or apprenticing by competent adults.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(c_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'functionals'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(tokensl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'short life expectancy'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.decode(r_sample.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflected = inference(c_sample, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(three'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflectedl = reflected.tolist()[0]\n",
    "special_decode(reflectedl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input = 'On what date is the Feat of Transfiguration celebrated?Ecclesiam suam was given at St. Peter\\'s, Rome, on the Feast of the Transfiguration, 6 August 1964, the second year of his Pontificate. It is considered an important document, identifying the Catholic Church with the Body of Christ. A later Council document Lumen Gentium stated that the Church subsists in the Body of Christ, raising questions as to the difference between \"is\" and \"subsists in\". Paul VI appealed to \"all people of good will\" and discussed necessary dialogues within the Church and between the Churches and with atheism.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = vocab.tokenize(custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded from uint16 to int32\n",
    "encoded = encoded.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = torch.tensor(encoded).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_output = inference(encoded.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'146'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_decode(custom_output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
